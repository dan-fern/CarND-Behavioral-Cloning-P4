# **Behavioral Cloning**

### Submissions: [`./model.py`](./model.py) & [`./models/model.h5`](./models/model.h5)

[//]: # (Image References)

[image1]:  ./res/nVidia.png             "Nvidia End-to-End CNN Model"
[image2]:  ./res/center.jpg             "Example Center Camera Image"

[video1]:  ./res/project_video_1.gif    "Video 1"
[video2]:  ./res/project_video_2.gif    "Video 2"
[video3]:  ./res/project_video_3.gif    "Video 3"
[video4]:  ./res/project_video_4.gif    "Video 4"
[video5]:  ./res/project_video_5.gif    "Video 5"
[video6]:  ./res/project_video_6.gif    "Video 6"
[video7]:  ./res/project_video_7.gif    "Video 7"
[video8]:  ./res/project_video_8.gif    "Video 8"
[video9]:  ./res/project_video_9.gif    "Video 9"

---

**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report

## [Rubric Points](https://review.udacity.com/#!/rubrics/432/view)

---

### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one. You can submit your writeup as markdown or pdf.

You're reading it!

### Files Submitted & Code Quality

**NOTE**: This project uses a Unity-based driving simulator to generate training data.  Neither the simulator, nor the data is included (although paths still remain in the python scripts).

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files (all linked below):
* [`./model.py`](./model.py) containing the script to create and train the model [[_File_](./model.py)]
* [`./drive.py`](./drive.py) for driving the car in autonomous mode [[_File_](./drive.py)]
* [`./models/model.h5`](./models/model.h5) containing a trained convolution neural network [[_File_](./models/model.h5)]
* [`./writeup.md`](./writeup.md) summarizing the results [[_File_](./writeup.md)]

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing:

```bash
python drive.py ./models/model.h5
```

**NOTE**: The `drive.py` file has a preset speed of 9mph.  To use the speed-trained model to have the car drive at more competitive speeds, run `drive_challenge.py` by executing:

```bash
python drive_challenge.py ./models/model.h5 ./models/speed.h5
```

#### 3. Submission code is usable and readable

The [`model.py`](./model.py) file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model.

### Model Architecture and Training Strategy

#### <a name="arch"></a> 1. An appropriate model architecture has been employed

The lecture notes suggest starting from a [known self-driving car CNN model developed by Nvidia](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf).  See below for a diagram below is a depicting the model architecture:

<center>![alt text][image1]</center>

<br><a name="preprocess"></a>I reproduced this model as shown above.  According to the paper, the image input is 200 x 66 pixels and is in the YUV color space.  When images are passed to the model, they are preprocessed accordingly.  Prior to that though, the image is cropped to remove superfluous information as recommended by the lecture notes.  A subtle Gaussian Blur is added to compensate for the model attempting to fit to features generated by poor graphics rendering.

```python
def preprocess_image( path ):

    image = cv2.imread( path )[60:140,:,:]
    image = cv2.GaussianBlur( image, ( 3, 3 ), 0 )
    image = cv2.resize( image, ( 200, 66 ), interpolation=cv2.INTER_AREA )
    image = cv2.cvtColor( image, cv2.COLOR_BGR2YUV )

    return image
```

After preprocessing, the image is normalized using the Keras Lambda function.  After that and according to the paper, I added three 5 x 5 convolution layers, two 3 x 3 convolution layers, and three fully-connected layers with 2 x 2 striding on the 5 x 5 convolutional layers.  The paper does not mention activation functions, dropout, or other attempts to reduce overfitting, so I began with ReLU activation on each fully-connected layer.  I chose the Adam optimizer and MSE loss function as recommended.  The final layer ("output" in the diagram) is a fully-connected layer with a single neuron.  The code for the full model is shown below:

```python
def train_model_0( x, y, epochs=11, save_model=True, model_name='model' ):

    model = Sequential( )

    model.add( Lambda( lambda x: x / 255.0 - 0.5, input_shape=( 66, 200, 3 ) ) )

    model.add( Convolution2D( 24, 5, 5,
            subsample=(2, 2), border_mode='valid', W_regularizer=l2(0.0005) ) )
    model.add( ELU( ) )

    model.add( Convolution2D( 36, 5, 5,
            subsample=(2, 2), border_mode='valid', W_regularizer=l2(0.0005) ) )
    model.add( ELU( ) )

    model.add( Convolution2D( 48, 5, 5,
            subsample=(2, 2), border_mode='valid', W_regularizer=l2(0.0005) ) )
    model.add( ELU( ) )

    model.add( Convolution2D( 64, 3, 3, border_mode='valid', W_regularizer=l2(0.0005) ) )
    model.add( ELU( ) )

    model.add( Convolution2D( 64, 3, 3, border_mode='valid', W_regularizer=l2(0.0005) ) )
    model.add( ELU( ) )

    model.add( Flatten( ) )

    model.add( Dense( 100, W_regularizer=l2(0.0005) ) )
    model.add( ELU( ) )

    model.add( Dense( 50, W_regularizer=l2(0.0005) ) )
    model.add( ELU( ) )

    model.add( Dense( 10, W_regularizer=l2(0.0005) ) )
    model.add( ELU( ) )

    model.add( Dense( 1 ) )

    model.compile( loss='mse', optimizer='adam' )

    model.fit( x, y, validation_split=0.2, shuffle=True, nb_epoch=epochs )

    if not save_model:
        return

    model.save( './models/' + model_name + '.h5' )

    return
```

Eventually, Dropout was added (and removed), L2 regularization with (λ=0.0005) was introduced, and ReLU activations were replaced with ELU activations for all layers, as recommended by the Udacity community.

#### 2. Attempts to reduce overfitting in the model

After initial training trials, I employed a few strategies to reduce overfitting.  These came mostly as community recommendations and were not implemented in any order:
* Removing dropout layers - _attempted, then removed_
* Adding L2 regularization (lambda of 0.0005) to all layers - _λ was tweaked throughout_
* Removing ReLU activations and adding ELU activations to all layers

This helped reduce bouncing back and forth between the sides of the road, but the better solution would have been smoother steering values in the training data.

As shown below, the model was trained and validated on shuffled and split data sets.  The validation set was split from 20% of the training data.  The model was tested qualitatively by running it through the simulator and ensuring that the vehicle could stay on the track.

```python
model.fit( x, y, validation_split=0.2, shuffle=True, nb_epoch=epochs )
```

#### 3. Model parameter tuning

The model used an adam optimizer, so the learning rate was not tuned manually.  Some tuned parameters include:
* `steering_correction_factor` <-- 0.22
* `W_regularizer` <-- `l2(0.0005)`
* removing mirrored images with `steering_angles < 0.01` (_eventually re-added_)

#### <a name="training"></a> 4. Appropriate training data

Training data was chosen to keep the vehicle driving on the road.  Udacity provides a dataset that can be used alone to produce a working model, but students are recommended to collect their own.  I used a combination of center lane driving, race line driving, and recovery driving from both sides of the road.  I emphasized using the mouse to steer wherever possible since the mouse produced the most gentle steering angle curves.  Surprisingly, I found this easiest on tighter turns than I did in gently-curving open roads, where steering angles less than 1° were required.  As a result, the vehicle model struggled the most along these portions of the road, where I was forced to use the "a" and "d" keys.

In addition, I collected some race-line data where the car hugs the outside line of a turn, uses the width of the road through the inner apex at the turn's midpoint, and then again using the width of the road back to the outside line of the turn while aggressively accelerating.  I did this because as a challenge, I planned to input a speed-trained model and wanted the car to learn to drive more competitively.  A sample image is shown below:

<center>![alt text][image2]</center>

<br>Udacity recommends collecting recovery data, where data is captured from the point of approaching the track edge and through the recovery maneuver of steering the car back toward the center of the track.  This gives the model the opportunity to learn how to recover to center.  This task may seem trivial to a human driver, but an untrained model requires data such as this to know how to react.  

### Model Architecture and Training Strategy

#### 1. Solution Design Approach

The overall strategy for deriving a model architecture was to _read the Nvidia white paper "[End to End Learning for Self-Driving Cars](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf)"_

My first step was to use a convolution neural network model similar to the _white paper_.  I thought this model might be appropriate because _the lecture notes made it seem like a good architecture_.

In order to gauge how well the model was working, _I qualitatively monitored the car driving to see if it stayed on the road_.

To combat the overfitting, I modified the model _according to the log in_ [`notes.txt`](./notes.txt).

Then, I _had a snack_.

The final step was to run the simulator to see how well the car was driving around track one.  _After implementing the first version of the Nvidia archtitecture, the vehicle never fell off the track.  From there, I attempted to optimize its driving behavior by tweaking the parameters listed in_ [`notes.txt`](./notes.txt).

At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road.

#### 2. Final Model Architecture

The final model architecture consisted of a convolution neural network with the following layers and layer sizes _[as described in the section above](#arch)_.

#### 3. Creation of the Training Set & Training Process

To capture good driving behavior, I _collected data [as described in the section above](#training)_.

To augment the data set, I also flipped images and angles thinking that this would _double the training set and balance left turns with right ones.  Here is what that code in_ `model.py` _looks like_:

```python
for file, i in zip( files, range( 3 ) ):

    images.append( preprocess_image( file ) )
    steers.append( steering_angles[i] )
    throts.append( throttle_value )
    speeds.append( speed_value )

    images.append( cv2.flip( images[-1], 1 ) )
    steers.append( steering_angles[i] * -1.0 )
    throts.append( throttle_value )
    speeds.append( speed_value )
```

After the collection process, I had _55650_ number of data points. I then preprocessed this data by _[way of the process described above](#preprocess)_.

I used this training data for training the model.

### Results

#### 1. Steering-Trained Model

The full video for the steering-trained model ([`./models/model.h5`](./models/model.h5)) can be found at [`./videos/video.mp4`](./videos/video.mp4).  Selected clips are viewable below:

<center>![alt text][video1]</center><br>

As mentioned above, once the final model was completed, the biggest hurdles were gently-curving portions of track.  This was because it was difficult to maintain low steering angles (less than 1°) for prolonged periods.  As a result, I had to often center the steering column (release the mouse) and use the "a" and "d" keys.  Not having a consistent curve of steering angle values leads to the vehicle to waffle between turning and straightening as shown in the video above.  

<center>![alt text][video2]</center><br>

When training, steeper turns proved easier to turn with the mouse.  As a result, the model learned these curves more effectively as shown above.

<center>![alt text][video3]</center><br>

Straighter portions of track tended to show more waffling, since most corrections in training were made using the "a" and "d" keys.  The bridge portion shown above is a good example of this behavior.

<center>![alt text][video4]</center><br>

But again, the model handled curves well since there was a steady curve of steering angle data to train upon.  The above shows the model effectively handling the most treacherous part of the track.  

<center>![alt text][video5]</center><br>

Likewise, the final curve is handled well.

#### 2. Steering-Trained Model with Speed-Trained Model

In addition, I challenged myself to have the car drive more aggressively around the track by employing a speed-trained model, [`./models/speed.h5`](./models/speed.h5) in addition to [`./models/model.h5`](./models/speed.h5).  To use this, I made some modifications to `drive.py` and saved them under `drive_challenge.py`.  To run this model, execute:

```bash
python drive_challenge.py ./models/model.h5 ./models/speed.h5
```

Some of the simple modifications to the `SimplePIController` class in `drive.py` are shown below:

```python
controller.set_desired( float( model_t.predict( image_array[None, :, :, :], batch_size=1 ) ) )

throttle = controller.update( float( speed ) )
```

Though low-speed issues are magnified, the car never leaves the track so I consider it a successful model.  The full video for the steering-trained and speed-trained models can be found at [`./videos/challenge.mp4`](./videos/challenge.mp4).  Selected clips are viewable below:

<center>![alt text][video6]</center><br>

The issue with gently-curving portions of track is magnified once the car is able to drive at full speed.  Not having a consistent curve of steering angle values leads to the vehicle to waffle between turning and straightening as shown in the video above.  

<center>![alt text][video7]</center><br>

This waffling is more amplified as the car exits a turn since there was race-line data added to the training sets.  Though the car stays on the track, I would consider this behavior unsafe since it is accelerating aggressively while waffling which can induce skidding or instability at the tires.  The bridge portion shown above is a good example of this behavior.

<center>![alt text][video8]</center><br>

But again, the model handled curves well since there was a steady curve of steering angle data to train upon.  What is most interesting is the steep deceleration the car experiences prior to a turn, not dissimilar to what a racecar driver might do.  Then, the car accelerates back to max speed upon exiting the curve.  The above shows the model effectively handling the most treacherous part of the track.  

<center>![alt text][video9]</center><br>

Likewise, the final curve is handled well.  

#### 3. Track No. 2

I did not attempt to drive on track #2 because of time constraints.  Sorry!! 

### Discussion

This success of this model (like much of machine learning) was tied to the quality and breadth of the training data collected; a machine learner will only learn as well as the data it trained upon.  Tuning the model never seemed to have as good an impact as adding more simulation data.

One way that I would like to improve my implementation is by collecting more consistent data.  Specifically, if I had more time, I would have used a steering wheel controller, or at least focused more effort to make all turns using only the mouse.  This would help the jittering that I experienced on straightaways.  I would also like to revisit the image preprocessing.  It's possible that more intuitive cropping or filtering might produce different results.  

On a personal note, I enjoyed this project greatly and am pleased with the results.  After the first implementation of the Nvidia architecture with no tuning, the car was already driving successfully around the track.  Training the car to drive itself with such relatively little effort was not just rewarding, but it proves how powerful deep learning (and deep learning libraries like Keras) have become.
